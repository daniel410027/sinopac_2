import pandas as pd
import numpy as np
import catboost as cb
import joblib
import json
import matplotlib.pyplot as plt
import os
import random
from deap import base, creator, tools, algorithms

from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, classification_report, confusion_matrix
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

# ======== åœ–è¡¨ä¸­æ–‡å­—é«”è¨­å®š ========
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'Microsoft YaHei', 'Heiti TC', 'STFangsong']
plt.rcParams['axes.unicode_minus'] = False

# ======== è®€å–è³‡æ–™ ========
df = pd.read_csv("../../database/filter_training.csv", encoding="utf-8")
# å–å¾—æ¬„ä½åˆ—è¡¨
columns = df.columns.tolist()

# åˆ‡å‡ºæ¬„ä½
first_column = columns[:1]  # ç¬¬ä¸€æ¬„
middle_column = columns[1:-1]  # ä¸­é–“æ¬„ä½ï¼ˆä¸å«é¦–å°¾ï¼‰
last_column = columns[-1:]  # æœ€å¾Œä¸€æ¬„

# é¸å– middle ä¸­çš„å‰ 120 æ¬„
middle_column_120 = middle_column[:120]

# é‡æ–°çµ„åˆæ¬„ä½é †åº
selected_columns = first_column + middle_column_120 + last_column

# å»ºç«‹æ–°çš„ DataFrame
df = df[selected_columns]
target = df["é£†è‚¡"]
features_raw = df.drop(["ID", "é£†è‚¡"], axis=1)
features_numeric = features_raw.select_dtypes(include=["number"])

# ======== æ¨™æº–åŒ–è™•ç†ï¼ˆå¯é¸ï¼‰ ========
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features_numeric)

# ======== ç¼ºå¤±å€¼è™•ç† ========
# ç§»é™¤å…¨ç‚º NaN çš„æ¬„ä½
cols_all_nan = features_numeric.columns[features_numeric.isna().all()].tolist()
print(f"ç§»é™¤å…¨éƒ¨ç‚ºç¼ºå¤±å€¼çš„æ¬„ä½: {cols_all_nan}")
features_numeric = features_numeric.drop(columns=cols_all_nan)

# ä½¿ç”¨ä¸­ä½æ•¸å¡«è£œç¼ºå¤±å€¼
imputer = SimpleImputer(strategy='median')
features_imputed = imputer.fit_transform(features_numeric)
features_clean = pd.DataFrame(features_imputed, columns=features_numeric.columns)

# ======== é¡åˆ¥ä¸å¹³è¡¡æ¯”ä¾‹è¨ˆç®— ========
positive = (target == 1).sum()
negative = (target == 0).sum()
scale_pos_weight = negative / positive
print(f"âœ… scale_pos_weight è¨ˆç®—å®Œæˆï¼š{scale_pos_weight:.2f}")

# ======== åˆ‡åˆ†è¨“ç·´èˆ‡é©—è­‰é›† ========
X_train, X_val, y_train, y_val = train_test_split(features_clean, target, test_size=0.2, random_state=62)

# ======== æº–å‚™CatBoostæ•¸æ“šé›† ========
train_data = cb.Pool(X_train, label=y_train)
eval_data = cb.Pool(X_val, label=y_val)

# ======== è¨­ç½®éºå‚³æ¼”ç®—æ³•åƒæ•¸ç¯„åœ ========
param_ranges = {
    'iterations': (100, 2000),
    'learning_rate': (0.01, 0.3),
    'depth': (4, 10),
    'l2_leaf_reg': (0.1, 10.0),
    'border_count': (32, 255),
    'bagging_temperature': (0.0, 1.0),
    'random_strength': (0.1, 10.0)
}

# ======== å»ºç«‹éºå‚³æ¼”ç®—æ³•å·¥å…· ========
creator.create("FitnessMax", base.Fitness, weights=(1.0,))
creator.create("Individual", list, fitness=creator.FitnessMax)

toolbox = base.Toolbox()

# å®šç¾©åŸºå› ç·¨ç¢¼ï¼ˆæ¯å€‹åƒæ•¸ç·¨ç¢¼ç‚ºä¸€å€‹åŸºå› ï¼‰
toolbox.register("attr_iterations", random.randint, param_ranges['iterations'][0], param_ranges['iterations'][1])
toolbox.register("attr_learning_rate", random.uniform, param_ranges['learning_rate'][0], param_ranges['learning_rate'][1])
toolbox.register("attr_depth", random.randint, param_ranges['depth'][0], param_ranges['depth'][1])
toolbox.register("attr_l2_leaf_reg", random.uniform, param_ranges['l2_leaf_reg'][0], param_ranges['l2_leaf_reg'][1])
toolbox.register("attr_border_count", random.randint, param_ranges['border_count'][0], param_ranges['border_count'][1])
toolbox.register("attr_bagging_temp", random.uniform, param_ranges['bagging_temperature'][0], param_ranges['bagging_temperature'][1])
toolbox.register("attr_random_strength", random.uniform, param_ranges['random_strength'][0], param_ranges['random_strength'][1])

# å‰µå»ºå€‹é«”å’Œæ—ç¾¤
toolbox.register("individual", tools.initCycle, creator.Individual,
                 (toolbox.attr_iterations, toolbox.attr_learning_rate, toolbox.attr_depth, 
                  toolbox.attr_l2_leaf_reg, toolbox.attr_border_count, toolbox.attr_bagging_temp,
                  toolbox.attr_random_strength), n=1)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

# ======== å®šç¾©è©•ä¼°å‡½æ•¸ ========
def evaluate_model(individual):
    iterations, learning_rate, depth, l2_leaf_reg, border_count, bagging_temp, random_strength = individual
    
    params = {
        "iterations": int(iterations),
        "learning_rate": learning_rate,
        "depth": int(depth),
        "l2_leaf_reg": l2_leaf_reg,
        "border_count": int(border_count),
        "bagging_temperature": bagging_temp,
        "random_strength": random_strength,
        "scale_pos_weight": scale_pos_weight,
        "loss_function": "Logloss",
        "eval_metric": "F1",
        "random_seed": 62,
        "verbose": False
    }
    
    try:
        model = cb.CatBoost(params)
        model.fit(train_data, eval_set=eval_data, early_stopping_rounds=50, verbose=False)
        
        y_val_proba = model.predict(X_val, prediction_type="Probability")[:, 1]
        
        # æœ€ä½³é–¾å€¼æœå°‹
        thresholds = np.linspace(0.1, 0.9, 9)
        best_f1, best_threshold = 0, 0.5
        for threshold in thresholds:
            y_pred = (y_val_proba > threshold).astype(int)
            f1 = f1_score(y_val, y_pred)
            if f1 > best_f1:
                best_f1, best_threshold = f1, threshold
                
        # ä¿å­˜è©²å€‹é«”çš„é–¾å€¼å’Œæœ€ä½³è¿­ä»£æ¬¡æ•¸
        individual.best_threshold = best_threshold
        individual.best_iteration = model.get_best_iteration()
        individual.f1_score = best_f1
        
        return (best_f1,)
    except Exception as e:
        print(f"è©•ä¼°éŒ¯èª¤: {e}")
        return (0.0,)

toolbox.register("evaluate", evaluate_model)
toolbox.register("mate", tools.cxBlend, alpha=0.5)
toolbox.register("mutate", tools.mutPolynomialBounded, low=[param_ranges[k][0] for k in param_ranges], 
                 up=[param_ranges[k][1] for k in param_ranges], eta=20.0, indpb=0.2)
toolbox.register("select", tools.selTournament, tournsize=3)

# ======== åŸ·è¡Œéºå‚³æ¼”ç®—æ³• ========
print("ğŸ§¬ é–‹å§‹éºå‚³æ¼”ç®—æ³•æœå°‹...")
population = toolbox.population(n=30)
hof = tools.HallOfFame(1)
stats = tools.Statistics(lambda ind: ind.fitness.values)
stats.register("avg", np.mean)
stats.register("min", np.min)
stats.register("max", np.max)
stats.register("std", np.std)

population, logbook = algorithms.eaSimple(population, toolbox, cxpb=0.7, mutpb=0.2, ngen=15, 
                                          stats=stats, halloffame=hof, verbose=True)

# å–å¾—æœ€ä½³å€‹é«”
best_ind = hof[0]
best_params = {
    "iterations": int(best_ind.best_iteration),  # ä½¿ç”¨æ—©åœæ‰¾åˆ°çš„æœ€ä½³è¿­ä»£æ¬¡æ•¸
    "learning_rate": best_ind[1],
    "depth": int(best_ind[2]),
    "l2_leaf_reg": best_ind[3],
    "border_count": int(best_ind[4]),
    "bagging_temperature": best_ind[5],
    "random_strength": best_ind[6],
    "scale_pos_weight": scale_pos_weight,
    "loss_function": "Logloss",
    "eval_metric": "F1",
    "random_seed": 62,
    "verbose": False
}

best_threshold = best_ind.best_threshold
best_f1 = best_ind.f1_score

print("\nğŸ¥‡ æœ€ä½³åƒæ•¸ï¼š", best_params)
print(f"ğŸ¯ æœ€ä½³é–¾å€¼ï¼š{best_threshold:.2f}")
print(f"ğŸ“Š æœ€ä½³ F1 åˆ†æ•¸ï¼š{best_f1:.4f}")

# ======== æœ€ä½³æ¨¡å‹è¨“ç·´èˆ‡é æ¸¬ ========
best_model = cb.CatBoost(best_params)
best_model.fit(train_data, verbose=False)

y_val_proba = best_model.predict(X_val, prediction_type="Probability")[:, 1]
y_val_pred = (y_val_proba > best_threshold).astype(int)

# ======== å„²å­˜æ¨¡å‹èˆ‡é è™•ç†å™¨ ========
best_model.save_model("genetic_best_catboost.cbm")
joblib.dump(imputer, "genetic_imputer.pkl")
with open("genetic_model_info.json", "w") as f:
    json.dump({
        'best_params': {k: float(v) if isinstance(v, (int, float)) else v for k, v in best_params.items()},
        'best_threshold': float(best_threshold),
        'features': list(features_clean.columns),
        'scale_pos_weight': float(scale_pos_weight),
        'best_f1': float(best_f1)
    }, f, indent=4)

print("âœ… æ¨¡å‹èˆ‡è³‡è¨Šå·²å„²å­˜å®Œæˆ")

# ======== è©•ä¼°å ±å‘Šèˆ‡æ··æ·†çŸ©é™£ ========
print("\nğŸ“Š åˆ†é¡å ±å‘Šï¼š")
print(classification_report(y_val, y_val_pred))

print("\nğŸ” æ··æ·†çŸ©é™£ï¼š")
print(confusion_matrix(y_val, y_val_pred))

# ======== ç‰¹å¾µé‡è¦æ€§è¦–è¦ºåŒ– ========
feature_importance = pd.DataFrame({
    'feature': features_clean.columns,
    'importance': best_model.get_feature_importance()
}).sort_values('importance', ascending=False)

feature_importance.to_csv("feature_importance.csv", index=False)

plt.figure(figsize=(12, 8))
plt.barh(feature_importance.head(20)['feature'], feature_importance.head(20)['importance'])
plt.xlabel('é‡è¦æ€§')
plt.ylabel('ç‰¹å¾µ')
plt.title('Top 20 ç‰¹å¾µé‡è¦æ€§')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.savefig("genetic_feature_importance.png")
plt.close()
print("âœ… ç‰¹å¾µé‡è¦æ€§åœ–å·²å„²å­˜ç‚º genetic_feature_importance.png")

# ======== é æ¸¬æ©Ÿç‡åˆ†å¸ƒåœ– ========
plt.figure(figsize=(10, 6))
plt.hist(y_val_proba, bins=50, alpha=0.7, color='blue')
plt.axvline(x=best_threshold, color='red', linestyle='--', label=f'æœ€ä½³é–¾å€¼ ({best_threshold:.2f})')
plt.xlabel('é æ¸¬æ©Ÿç‡')
plt.ylabel('é »ç‡')
plt.title('é©—è­‰é›†é æ¸¬æ©Ÿç‡åˆ†å¸ƒ')
plt.legend()
plt.grid(True)
plt.savefig("genetic_prediction_distribution.png")
plt.close()
print("âœ… é æ¸¬æ¦‚ç‡åˆ†å¸ƒåœ–å·²å„²å­˜ç‚º genetic_prediction_distribution.png")

# ======== å­¸ç¿’æ›²ç·š ========
generations = range(len(logbook))
avg_fitness = [d['avg'] for d in logbook]
max_fitness = [d['max'] for d in logbook]

plt.figure(figsize=(10, 6))
plt.plot(generations, avg_fitness, 'b-', label='å¹³å‡é©æ‡‰åº¦')
plt.plot(generations, max_fitness, 'r-', label='æœ€ä½³é©æ‡‰åº¦')
plt.xlabel('ä¸–ä»£')
plt.ylabel('F1 åˆ†æ•¸')
plt.title('éºå‚³æ¼”ç®—æ³•å­¸ç¿’æ›²ç·š')
plt.legend()
plt.grid(True)
plt.savefig("genetic_learning_curve.png")
plt.close()
print("âœ… éºå‚³æ¼”ç®—æ³•å­¸ç¿’æ›²ç·šå·²å„²å­˜ç‚º genetic_learning_curve.png")