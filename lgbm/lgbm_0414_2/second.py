import pandas as pd
import numpy as np
import lightgbm as lgb
import optuna
import joblib
import json
import matplotlib.pyplot as plt
import os

from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, classification_report, confusion_matrix
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler


# ======== åœ–è¡¨ä¸­æ–‡å­—é«”è¨­å®š ========
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'Microsoft YaHei', 'Heiti TC', 'STFangsong']
plt.rcParams['axes.unicode_minus'] = False

# ======== è®€å–è³‡æ–™ ========
df = pd.read_csv("../../database/filter_training.csv", encoding="utf-8")
target = df["é£†è‚¡"]
features_raw = df.drop(["ID", "é£†è‚¡"], axis=1)
features_numeric = features_raw.select_dtypes(include=["number"])

# ======== æ¨™æº–åŒ–è™•ç†ï¼ˆå¯é¸ï¼‰ ========
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features_numeric)

# ======== ç¼ºå¤±å€¼è™•ç†ï¼ˆWinsorizeç•¥éï¼Œé€™è£¡ä¿ç•™åŸå§‹ï¼‰ ========
# ç§»é™¤å…¨ç‚º NaN çš„æ¬„ä½
cols_all_nan = features_numeric.columns[features_numeric.isna().all()].tolist()
print(f"ç§»é™¤å…¨éƒ¨ç‚ºç¼ºå¤±å€¼çš„æ¬„ä½: {cols_all_nan}")
features_numeric = features_numeric.drop(columns=cols_all_nan)

# ä½¿ç”¨ä¸­ä½æ•¸å¡«è£œç¼ºå¤±å€¼
imputer = SimpleImputer(strategy='median')
features_imputed = imputer.fit_transform(features_numeric)
features_clean = pd.DataFrame(features_imputed, columns=features_numeric.columns)

# ======== é¡åˆ¥ä¸å¹³è¡¡æ¯”ä¾‹è¨ˆç®— ========
positive = (target == 1).sum()
negative = (target == 0).sum()
scale_pos_weight = negative / positive
print(f"âœ… scale_pos_weight è¨ˆç®—å®Œæˆï¼š{scale_pos_weight:.2f}")

# ======== åˆ‡åˆ†è¨“ç·´èˆ‡é©—è­‰é›† ========
X_train, X_val, y_train, y_val = train_test_split(features_clean, target, test_size=0.2, random_state=62)

# ======== è‡ªå®šç¾© F1 è©•ä¼°å‡½æ•¸ï¼ˆå¯ç”¨æ–¼ CV è©•ä¼°ï¼‰ ========
def lgbm_f1_score(y_true, y_pred):
    y_pred_binary = np.round(y_pred)
    return 'f1', f1_score(y_true, y_pred_binary), True

# ======== Optuna ç›®æ¨™å‡½æ•¸ ========
def objective(trial):
    params = {
        "objective": "binary",
        "boosting_type": "gbdt",
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
        "n_estimators": trial.suggest_int("n_estimators", 100, 2000),
        "max_depth": trial.suggest_int("max_depth", 3, 12),
        "num_leaves": trial.suggest_int("num_leaves", 15, 255),
        "subsample": trial.suggest_float("subsample", 0.6, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
        "min_child_samples": trial.suggest_int("min_child_samples", 10, 100),
        "reg_alpha": trial.suggest_float("reg_alpha", 0.0, 10.0),
        "reg_lambda": trial.suggest_float("reg_lambda", 0.0, 10.0),
        "scale_pos_weight": scale_pos_weight,
        "max_bin": trial.suggest_int("max_bin", 255, 1024),
        "random_state": 62,
        "verbose": -1
    }

    model = lgb.LGBMClassifier(**params)
    model.fit(X_train, y_train)

    y_val_proba = model.predict_proba(X_val)[:, 1]

    # æœ€ä½³é–¾å€¼æœå°‹
    thresholds = np.linspace(0.1, 0.9, 9)
    best_f1, best_threshold = 0, 0.5
    for threshold in thresholds:
        y_pred = (y_val_proba > threshold).astype(int)
        f1 = f1_score(y_val, y_pred)
        if f1 > best_f1:
            best_f1, best_threshold = f1, threshold

    trial.set_user_attr('best_threshold', best_threshold)
    return best_f1


### ä¸‹ä¸€ä»½ä¸ç”¨é€™å€‹
# ======== æ¸¬è©¦ä¸åŒç‰¹å¾µæ•¸é‡å° F1 çš„å½±éŸ¿ï¼ˆæ¯æ¬¡éƒ½ç”¨ Optuna é‡æ–°èª¿åƒï¼‰+ ä¸­é–“å„²å­˜ ========


save_path = "f1_scores_by_feature_count.json"
f1_scores_by_feature_count = {}

# å˜—è©¦è®€å–å·²å­˜åœ¨çš„ä¸­é–“çµæœ
if os.path.exists(save_path):
    with open(save_path, "r") as f:
        f1_scores_by_feature_count = json.load(f)
    print(f"ğŸ“‚ å·²è®€å–ä¸­é–“çµæœï¼Œå…±æœ‰ {len(f1_scores_by_feature_count)} ç­†")
else:
    print("ğŸ“ å°šç„¡ä¸­é–“çµæœï¼Œå¾é ­é–‹å§‹")

feature_importance = pd.read_csv("feature_importance.csv")

feature_counts = list(range(100, min(251, len(feature_importance)+1), 1))
print("\nğŸš€ é–‹å§‹æ¸¬è©¦ä¸åŒç‰¹å¾µæ•¸é‡å° F1 åˆ†æ•¸çš„å½±éŸ¿ï¼ˆæ”¯æ´ä¸­é€”å­˜æª”èˆ‡çºŒè·‘ï¼‰...")

for top_n in feature_counts:
    key = str(top_n)
    if key in f1_scores_by_feature_count:
        print(f"â© ç‰¹å¾µæ•¸: {top_n} å·²è¨ˆç®—éï¼Œç•¥é")
        continue

    selected_features = feature_importance['feature'].head(top_n).tolist()
    X_train_sub = X_train[selected_features]
    X_val_sub = X_val[selected_features]

    def objective_sub(trial):
        params = {
            "objective": "binary",
            "boosting_type": "gbdt",
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
            "n_estimators": trial.suggest_int("n_estimators", 100, 2000),
            "max_depth": trial.suggest_int("max_depth", 3, 12),
            "num_leaves": trial.suggest_int("num_leaves", 15, 255),
            "subsample": trial.suggest_float("subsample", 0.6, 1.0),
            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
            "min_child_samples": trial.suggest_int("min_child_samples", 10, 100),
            "reg_alpha": trial.suggest_float("reg_alpha", 0.0, 10.0),
            "reg_lambda": trial.suggest_float("reg_lambda", 0.0, 10.0),
            "scale_pos_weight": scale_pos_weight,
            "max_bin": trial.suggest_int("max_bin", 255, 1024),
            "random_state": 62,
            "verbose": -1
        }

        model = lgb.LGBMClassifier(**params)
        model.fit(X_train_sub, y_train)

        y_pred_proba = model.predict_proba(X_val_sub)[:, 1]
        thresholds = np.linspace(0.1, 0.9, 9)
        best_f1 = 0

        for threshold in thresholds:
            y_pred = (y_pred_proba > threshold).astype(int)
            f1 = f1_score(y_val, y_pred)
            if f1 > best_f1:
                best_f1 = f1

        return best_f1

    study_sub = optuna.create_study(direction="maximize")
    study_sub.optimize(objective_sub, n_trials=10, show_progress_bar=False)

    best_f1 = study_sub.best_value
    f1_scores_by_feature_count[key] = best_f1
    print(f"âœ… ç‰¹å¾µæ•¸: {top_n}ï¼Œæœ€ä½³ F1 Score: {best_f1:.4f}")

    # æ¯æ¬¡è·‘å®Œå°±å„²å­˜ä¸€æ¬¡
    with open(save_path, "w") as f:
        json.dump(f1_scores_by_feature_count, f, indent=4)

# ======== ç¹ªåœ–ï¼šç‰¹å¾µæ•¸ vs F1 Score ========
feature_counts_done = [int(k) for k in f1_scores_by_feature_count.keys()]
f1_scores_done = [f1_scores_by_feature_count[k] for k in f1_scores_by_feature_count]

plt.figure(figsize=(10, 6))
plt.plot(feature_counts_done, f1_scores_done, marker='o')
plt.xlabel("ä½¿ç”¨çš„ç‰¹å¾µæ•¸é‡")
plt.ylabel("F1 åˆ†æ•¸")
plt.title("ä¸åŒç‰¹å¾µæ•¸é‡å° F1 Score çš„å½±éŸ¿")
plt.grid(True)
plt.savefig("f1_score_by_feature_count.png")
plt.close()
print("ğŸ“ˆ ç‰¹å¾µæ•¸é‡ vs F1 score åœ–å·²å„²å­˜ç‚º f1_score_by_feature_count.png")
