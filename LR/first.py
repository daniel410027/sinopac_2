import pandas as pd
import os
import optuna
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
import numpy as np
import json
import shap

# ğŸ“ è³‡æ–™å¤¾è·¯å¾‘
data_dir = "../LR_database"
output_dir = "../LR_database/lr_results"
imputed_dir = "../LR_database/imputed_data"
os.makedirs(output_dir, exist_ok=True)
os.makedirs(imputed_dir, exist_ok=True)

try:
    for i in range(1, 17):
        output_path = os.path.join(output_dir, f"feature_importance_{i}.json")
        
        # æª¢æŸ¥æ˜¯å¦å·²å®Œæˆ
        if os.path.exists(output_path):
            print(f"â­ å·²å­˜åœ¨ï¼Œè·³éï¼šfeature_importance_{i}.json")
            continue

        file_path = os.path.normpath(os.path.join(data_dir, f"filtered_number_{i}.csv"))
        print(f"ğŸš€ æ­£åœ¨è™•ç†ï¼š{file_path}")
        
        # è®€å–è³‡æ–™
        df = pd.read_csv(file_path)

        # æ‹†åˆ†æ¬„ä½
        X_raw = df.iloc[:, 1:-1]  # å»æ‰ ID èˆ‡ é£†è‚¡
        y = df.iloc[:, -1]
        feature_names = X_raw.columns.tolist()

        # ç¼ºå€¼å¡«è£œï¼ˆä¸­ä½æ•¸ï¼‰
        imputer = SimpleImputer(strategy='median')
        X_imputed = imputer.fit_transform(X_raw)

        # å„²å­˜å¡«è£œç¼ºå€¼ä½†å°šæœªæ¨™æº–åŒ–çš„è³‡æ–™
        df_imputed = pd.DataFrame(X_imputed, columns=feature_names)
        df_imputed.insert(0, 'ID', df.iloc[:, 0])         # é‚„åŸ ID
        df_imputed['é£†è‚¡'] = y                            # é‚„åŸ target
        df_imputed.to_csv(os.path.join(imputed_dir, f"imputed_{i}.csv"), index=False)
        print(f"ğŸ“„ å·²å„²å­˜å¡«è£œç¼ºå€¼è³‡æ–™ï¼šimputed_{i}.csv")

        # æ¨™æº–åŒ–
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X_imputed)

        # Optuna èª¿åƒ
        def objective(trial):
            C = trial.suggest_loguniform('C', 1e-3, 1e2)
            penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])
            solver = 'liblinear' if penalty == 'l1' else 'lbfgs'
            model = LogisticRegression(C=C, penalty=penalty, solver=solver, max_iter=1000)
            score = cross_val_score(model, X_scaled, y, cv=3, scoring='f1').mean()
            return score

        study = optuna.create_study(direction="maximize")
        study.optimize(objective, n_trials=10)
        
        best_params = study.best_params
        print(f"âœ… æœ€ä½³åƒæ•¸ï¼š{best_params}")
        
        # è¨“ç·´æœ€ä½³æ¨¡å‹
        final_model = LogisticRegression(**best_params, solver='liblinear' if best_params['penalty'] == 'l1' else 'lbfgs', max_iter=1000)
        final_model.fit(X_scaled, y)

        # ä½¿ç”¨ SHAP è¨ˆç®—ç‰¹å¾µé‡è¦æ€§
        explainer = shap.LinearExplainer(final_model, X_scaled, feature_perturbation="interventional")
        shap_values = explainer.shap_values(X_scaled)
        shap_importance = dict(zip(
            feature_names,
            np.abs(shap_values).mean(axis=0).tolist()
        ))

        # å„²å­˜ SHAP ç‰¹å¾µé‡è¦æ€§
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(shap_importance, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“¦ å·²å„²å­˜ SHAP feature_importance_{i}.json\n")

except KeyboardInterrupt:
    print("â›”ï¸ ä½¿ç”¨è€…ä¸­æ–·ç¨‹åºï¼Œå·²å®‰å…¨åœæ­¢")
except Exception as e:
    print(f"âš ï¸ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

print("ğŸ‰ è™•ç†å®Œæˆï¼ˆæˆ–å·²ä¸­æ–·ï¼‰")
