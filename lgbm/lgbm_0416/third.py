import pandas as pd
import numpy as np
import lightgbm as lgb
import optuna
import joblib
import json
import matplotlib.pyplot as plt
import os

from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, classification_report, confusion_matrix
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

# ======== åœ–è¡¨ä¸­æ–‡å­—é«”è¨­å®š ========
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'Microsoft YaHei', 'Heiti TC', 'STFangsong']
plt.rcParams['axes.unicode_minus'] = False

# ======== è®€å–è³‡æ–™ ========
df = pd.read_csv("../../database/lgbm_filter_training.csv", encoding="utf-8")
# å–å¾—æ¬„ä½åˆ—è¡¨
columns = df.columns.tolist()

# åˆ‡å‡ºæ¬„ä½
first_column = columns[:1]  # ç¬¬ä¸€æ¬„
middle_column = columns[1:-1]  # ä¸­é–“æ¬„ä½ï¼ˆä¸å«é¦–å°¾ï¼‰
last_column = columns[-1:]  # æœ€å¾Œä¸€æ¬„

# é¸å– middle ä¸­çš„å‰ 120 æ¬„
middle_column_120 = middle_column[:203]

# é‡æ–°çµ„åˆæ¬„ä½é †åº
selected_columns = first_column + middle_column_120 + last_column

# å»ºç«‹æ–°çš„ DataFrame
df = df[selected_columns]
target = df["é£†è‚¡"]
features_raw = df.drop(["ID", "é£†è‚¡"], axis=1)
features_numeric = features_raw.select_dtypes(include=["number"])

# ======== æ¨™æº–åŒ–è™•ç†ï¼ˆå¯é¸ï¼‰ ========
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features_numeric)

feature_numeric = features_scaled

# ======== ç¼ºå¤±å€¼è™•ç†ï¼ˆWinsorizeç•¥éï¼Œé€™è£¡ä¿ç•™åŸå§‹ï¼‰ ========
# ç§»é™¤å…¨ç‚º NaN çš„æ¬„ä½
cols_all_nan = features_numeric.columns[features_numeric.isna().all()].tolist()
print(f"ç§»é™¤å…¨éƒ¨ç‚ºç¼ºå¤±å€¼çš„æ¬„ä½: {cols_all_nan}")
features_numeric = features_numeric.drop(columns=cols_all_nan)

# ä½¿ç”¨ä¸­ä½æ•¸å¡«è£œç¼ºå¤±å€¼
imputer = SimpleImputer(strategy='median')
features_imputed = imputer.fit_transform(features_numeric)
features_clean = pd.DataFrame(features_imputed, columns=features_numeric.columns)

# ======== é¡åˆ¥ä¸å¹³è¡¡æ¯”ä¾‹è¨ˆç®— ========
positive = (target == 1).sum()
negative = (target == 0).sum()
scale_pos_weight = negative / positive
print(f"âœ… scale_pos_weight è¨ˆç®—å®Œæˆï¼š{scale_pos_weight:.2f}")

# ======== åˆ‡åˆ†è¨“ç·´èˆ‡é©—è­‰é›† ========
X_train, X_val, y_train, y_val = train_test_split(features_clean, target, test_size=0.2, random_state=62)

# ======== è‡ªå®šç¾© F1 è©•ä¼°å‡½æ•¸ï¼ˆå¯ç”¨æ–¼ CV è©•ä¼°ï¼‰ ========
def lgbm_f1_score(y_true, y_pred):
    y_pred_binary = np.round(y_pred)
    return 'f1', f1_score(y_true, y_pred_binary), True

# ======== Optuna ç›®æ¨™å‡½æ•¸ ========
def objective(trial):
    params = {
        "objective": "binary",
        "boosting_type": "gbdt",
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
        "n_estimators": trial.suggest_int("n_estimators", 100, 2000),
        "max_depth": trial.suggest_int("max_depth", 3, 12),
        "num_leaves": trial.suggest_int("num_leaves", 15, 255),
        "subsample": trial.suggest_float("subsample", 0.6, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
        "min_child_samples": trial.suggest_int("min_child_samples", 10, 100),
        "reg_alpha": trial.suggest_float("reg_alpha", 0.0, 10.0),
        "reg_lambda": trial.suggest_float("reg_lambda", 0.0, 10.0),
        "scale_pos_weight": scale_pos_weight,
        "max_bin": trial.suggest_int("max_bin", 255, 1024),
        "random_state": 62,
        "verbose": -1
    }

    model = lgb.LGBMClassifier(**params)
    model.fit(X_train, y_train)

    y_val_proba = model.predict_proba(X_val)[:, 1]

    # æœ€ä½³é–¾å€¼æœå°‹
    thresholds = np.linspace(0.1, 0.9, 9)
    best_f1, best_threshold = 0, 0.5
    for threshold in thresholds:
        y_pred = (y_val_proba > threshold).astype(int)
        f1 = f1_score(y_val, y_pred)
        if f1 > best_f1:
            best_f1, best_threshold = f1, threshold

    trial.set_user_attr('best_threshold', best_threshold)
    return best_f1

# ======== åŸ·è¡Œ Optuna è¶…åƒæ•¸æœå°‹ ========
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=50)

best_params = study.best_params
best_threshold = study.best_trial.user_attrs['best_threshold']
print("ğŸ¥‡ æœ€ä½³åƒæ•¸ï¼š", best_params)
print(f"ğŸ¯ æœ€ä½³é–¾å€¼ï¼š{best_threshold:.2f}")

# ======== æœ€ä½³æ¨¡å‹è¨“ç·´èˆ‡é æ¸¬ ========
best_params['scale_pos_weight'] = scale_pos_weight
best_model = lgb.LGBMClassifier(**best_params, random_state=62)
best_model.fit(X_train, y_train)

y_val_proba = best_model.predict_proba(X_val)[:, 1]
y_val_pred = (y_val_proba > best_threshold).astype(int)

# ======== é›™é–¾å€¼æœ€ä½³åŒ–æœå°‹èˆ‡ heatmap ç¹ªè£½ ========
import seaborn as sns

def find_best_dual_threshold(y_true, y_proba, plot_heatmap=True):
    thresholds = np.linspace(0.1, 0.9, 9)
    best_f1 = 0
    best_small = 0.1
    best_big = 0.9
    f1_matrix = np.zeros((len(thresholds), len(thresholds)))

    for i, small in enumerate(thresholds):
        for j, big in enumerate(thresholds):
            if big <= small:
                f1_matrix[i, j] = np.nan
                continue

            y_pred = np.full_like(y_true, fill_value=2)
            y_pred[y_proba > big] = 1
            y_pred[y_proba < small] = 0

            mask = y_pred != 2
            if mask.sum() == 0:
                f1_matrix[i, j] = np.nan
                continue

            f1 = f1_score(y_true[mask], y_pred[mask])
            f1_matrix[i, j] = f1

            if f1 > best_f1:
                best_f1 = f1
                best_small = small
                best_big = big

    if plot_heatmap:
        plt.figure(figsize=(10, 8))
        sns.heatmap(f1_matrix, xticklabels=thresholds.round(2), yticklabels=thresholds.round(2),
                    annot=True, fmt=".2f", cmap="YlGnBu")
        plt.xlabel("big_threshold")
        plt.ylabel("small_threshold")
        plt.title("F1-score Heatmap for Dual Thresholds")
        plt.tight_layout()
        plt.savefig("dual_threshold_heatmap.png")
        plt.close()
        print("âœ… é›™é–¾å€¼ F1-score heatmap åœ–å·²å„²å­˜ç‚º dual_threshold_heatmap.png")

    return best_small, best_big, best_f1

# åŸ·è¡Œé›™é–¾å€¼æœå°‹
best_small, best_big, best_f1_dual = find_best_dual_threshold(y_val.values, y_val_proba)
print(f"\nğŸ¯ é›™é–¾å€¼æœ€ä½³ F1-scoreï¼š{best_f1_dual:.4f}")
print(f"ğŸ”º small_threshold = {best_small:.2f}, big_threshold = {best_big:.2f}")

# ======== å„²å­˜æ¨¡å‹èˆ‡é è™•ç†å™¨ ========
joblib.dump(best_model, "optuna_best_lgbm.pkl")
joblib.dump(imputer, "optuna_imputer.pkl")
with open("optuna_model_info.json", "w") as f:
    json.dump({
        'best_params': {k: float(v) if isinstance(v, (int, float)) else v for k, v in best_params.items()},
        'best_threshold': float(best_threshold),
        'dual_threshold': {
            'small': float(best_small),
            'big': float(best_big),
            'f1': float(best_f1_dual)
        },
        'features': list(features_clean.columns),
        'scale_pos_weight': float(scale_pos_weight)
    }, f, indent=4)

print("âœ… æ¨¡å‹èˆ‡è³‡è¨Šå·²å„²å­˜å®Œæˆ")
